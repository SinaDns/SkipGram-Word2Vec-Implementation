{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29a03303",
   "metadata": {},
   "source": [
    "# Skip-Gram Word Embedding with Negative Sampling\n",
    "\n",
    "This notebook implements the Word2Vec algorithm using the **Skip-gram** architecture with **Negative Sampling** from scratch using TensorFlow/Keras.\n",
    "We use the **text8** dataset (Wikipedia dump) to learn dense vector representations of words.\n",
    "\n",
    "## Project Structure\n",
    "- **Data Loading**: Download and extract the text8 dataset.\n",
    "- **Preprocessing**: \n",
    "  - Tokenization, Lowercasing.\n",
    "  - Stopwords removal.\n",
    "  - Subsampling freqeunt words.\n",
    "- **Data Generation**: \n",
    "  - Skip-gram pair generation (Target, Context).\n",
    "  - Negative Sampling.\n",
    "- **Model**: Custom Keras Model subclassing `tf.keras.Model`.\n",
    "- **Training**: Custom training loop with `BinaryCrossentropy` loss.\n",
    "- **Visualization**: Exporting embeddings for Tensorflow Projector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55aeefe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import io\n",
    "import math\n",
    "import gzip\n",
    "import nltk\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gensim.downloader as api\n",
    "import tensorflow_datasets as tfds\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import skipgrams\n",
    "\n",
    "# Ensure stopwords are downloaded\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0233c55f",
   "metadata": {},
   "source": [
    "## 1. Data Loading\n",
    "Loading the **text8** dataset which consists of the first 100MB of clean Wikipedia text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63abbcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    text8_zip_file_path = api.load('text8', return_path=True)\n",
    "    with gzip.open(text8_zip_file_path, 'rb') as file:\n",
    "        file_content = file.read()\n",
    "    wiki = file_content.decode()\n",
    "    return wiki\n",
    "\n",
    "wiki = load_data()\n",
    "print(f\"Data Loaded. Length: {len(wiki)} characters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5c4674",
   "metadata": {},
   "source": [
    "## 2. Preprocessing\n",
    "We define a preprocessing pipeline that:\n",
    "1. Cleans punctuation.\n",
    "2. Removes stopwords (common words like \"the\", \"is\").\n",
    "3. Removes rare words (< 5 occurrences).\n",
    "4. Subsamples frequent words using Mikolov's heuristic: $P(w_i) = 1 - \\sqrt{\\frac{t}{f(w_i)}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6beec612",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Step 1: Clean punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    \n",
    "    # Step 2: Lowercase and strip\n",
    "    text = text.lower().strip()\n",
    "    \n",
    "    # Step 3: Stopwords removal\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Step 4: Minimum frequency filter\n",
    "    word_counts = Counter(words)\n",
    "    words = [word for word in words if word_counts[word] >= 5]\n",
    "    \n",
    "    # Step 5: Subsampling\n",
    "    total_words = sum(word_counts.values())\n",
    "    \n",
    "    def subsample_probability(word):\n",
    "        freq = word_counts[word] / total_words\n",
    "        # Heuristic from the paper\n",
    "        return 1 - (np.sqrt(1e-5 / freq) + 1e-5 / freq)\n",
    "        \n",
    "    subsampled_words = [word for word in words if random.random() > subsample_probability(word)]\n",
    "    \n",
    "    return subsampled_words, word_counts\n",
    "\n",
    "# Run preprocessing (might take a moment)\n",
    "preprocessed_words, word_counts = preprocess_text(wiki)\n",
    "print(f\"Preprocessing complete. Vocabulary size: {len(word_counts)}\")\n",
    "print(f\"Sample words: {preprocessed_words[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3207f4d7",
   "metadata": {},
   "source": [
    "## 3. Dataset Generation\n",
    "We use Keras `skipgrams` to generate positive pairs (Target, Context) and Negative pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc717301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "EMBEDDING_DIM = 100\n",
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 5\n",
    "WINDOW_SIZE = 5\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(preprocessed_words)\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(word_index) + 1\n",
    "\n",
    "# Generate Sequences\n",
    "sequences = tokenizer.texts_to_sequences([preprocessed_words])[0]\n",
    "\n",
    "# Generate Skip-grams\n",
    "print(\"Generating skip-gram pairs...\")\n",
    "pairs, labels = skipgrams(sequences, vocabulary_size=vocab_size, window_size=WINDOW_SIZE, negative_samples=0.75)\n",
    "\n",
    "# Prepare Arrays\n",
    "targets, contexts = zip(*pairs)\n",
    "targets = np.array(targets, dtype=np.int32)\n",
    "contexts = np.array(contexts, dtype=np.int32)\n",
    "labels = np.array(labels, dtype=np.int32)\n",
    "\n",
    "# Train/Test Split\n",
    "indices = np.arange(len(targets))\n",
    "np.random.shuffle(indices)\n",
    "split_idx = int(len(targets) * 0.8)\n",
    "\n",
    "train_indices = indices[:split_idx]\n",
    "test_indices = indices[split_idx:]\n",
    "\n",
    "# TF Datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(((targets[train_indices], contexts[train_indices]), labels[train_indices]))\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(((targets[test_indices], contexts[test_indices]), labels[test_indices]))\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "print(f\"Train/Test batches: {len(train_dataset)}, {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a905759b",
   "metadata": {},
   "source": [
    "## 4. Model Definition\n",
    "We define a custom Keras Model.\n",
    "- **Target Embedding**: Embedding for the center word.\n",
    "- **Context Embedding**: Embedding for the context word.\n",
    "- **Dot Product**: Similarity score.\n",
    "- **Sigmoid**: Probability output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f634a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        self.target_embedding = tf.keras.layers.Embedding(\n",
    "            input_dim=vocab_size, \n",
    "            output_dim=embedding_dim, \n",
    "            name=\"target_embedding\"\n",
    "        )\n",
    "        self.context_embedding = tf.keras.layers.Embedding(\n",
    "            input_dim=vocab_size, \n",
    "            output_dim=embedding_dim, \n",
    "            name=\"context_embedding\"\n",
    "        )\n",
    "        self.dot = tf.keras.layers.Dot(axes=-1, normalize=False, name=\"dot_product\")\n",
    "        self.output_layer = tf.keras.layers.Activation(\"sigmoid\", name=\"output\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        target, context = inputs\n",
    "        target_embed = self.target_embedding(target)\n",
    "        context_embed = self.context_embedding(context)\n",
    "        dot_product = self.dot([target_embed, context_embed])\n",
    "        return self.output_layer(dot_product)\n",
    "\n",
    "model = Word2Vec(vocab_size=vocab_size, embedding_dim=EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6a43e4",
   "metadata": {},
   "source": [
    "## 5. Training\n",
    "Custom training loop using `GradientTape`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277dbf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
    "train_acc_metric = tf.keras.metrics.BinaryAccuracy()\n",
    "val_acc_metric = tf.keras.metrics.BinaryAccuracy()\n",
    "\n",
    "@tf.function\n",
    "def train_step(model, optimizer, loss_fn, targets, x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model((targets, x), training=True)\n",
    "        loss = loss_fn(y, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
    "    train_acc_metric.update_state(y, predictions)\n",
    "    return loss\n",
    "\n",
    "@tf.function\n",
    "def test_step(model, loss_fn, targets, x, y):\n",
    "    predictions = model((targets, x), training=False)\n",
    "    loss = loss_fn(y, predictions)\n",
    "    val_acc_metric.update_state(y, predictions)\n",
    "    return loss\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(EPOCHS):\n",
    "    start_time = time.time()\n",
    "    train_acc_metric.reset_state()\n",
    "    val_acc_metric.reset_state()\n",
    "    \n",
    "    # Train\n",
    "    train_loss = 0.0\n",
    "    for step, ((targets, contexts), labels) in enumerate(train_dataset):\n",
    "        loss = train_step(model, optimizer, loss_fn, targets, contexts, labels)\n",
    "        train_loss += loss.numpy()\n",
    "    train_loss /= (step + 1)\n",
    "    \n",
    "    # Validation\n",
    "    val_loss = 0.0\n",
    "    for step, ((targets, contexts), labels) in enumerate(test_dataset):\n",
    "        loss = test_step(model, loss_fn, targets, contexts, labels)\n",
    "        val_loss += loss.numpy()\n",
    "    val_loss /= (step + 1)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} | \"\n",
    "          f\"Train Loss: {train_loss:.4f} Acc: {train_acc_metric.result():.4f} | \"\n",
    "          f\"Val Loss: {val_loss:.4f} Acc: {val_acc_metric.result():.4f} | \"\n",
    "          f\"Time: {time.time() - start_time:.2f}s\")\n",
    "\n",
    "# Save Checkmark\n",
    "checkpoint_path = \"./checkpoints/word2vec_ckpt\"\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)\n",
    "checkpoint.save(file_prefix=checkpoint_path)\n",
    "print(\"Model saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7eac8e",
   "metadata": {},
   "source": [
    "## 6. Embedding Export\n",
    "Exporting vectors for visualization in [TensorFlow Embedding Projector](http://projector.tensorflow.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1dbc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.target_embedding.get_weights()[0]\n",
    "\n",
    "with open(\"vecs.tsv\", \"w\", encoding='utf-8') as vecs_file, open(\"meta.tsv\", \"w\", encoding='utf-8') as meta_file:\n",
    "    for word, idx in tokenizer.word_index.items():\n",
    "        if idx < vocab_size: # Ensure index is within range\n",
    "            vec = weights[idx]\n",
    "            meta_file.write(f\"{word}\\n\")\n",
    "            vecs_file.write(\"\\t\".join(map(str, vec)) + \"\\n\")\n",
    "\n",
    "print(\"Export complete: vecs.tsv, meta.tsv\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
